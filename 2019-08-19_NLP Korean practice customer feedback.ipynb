{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_33yeT87Rf_"
   },
   "source": [
    "#  구글 드라이브 마운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qGxU_ZS8DEE"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-h1MVpOSRVl"
   },
   "source": [
    "# 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOHw8KJbRC4m"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git\n",
    "!pip install konlpy\n",
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3 \n",
    "!pip install torch\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzEpo9swkCZT"
   },
   "outputs": [],
   "source": [
    "# 데이터 처리 \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# 시각화 \n",
    "import seaborn as sns \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 시각화 옵션\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "# 한글 글꼴 설정\n",
    "mpl.rc('font',family ='Malgun Gothic')\n",
    "# 음수값 설정 \n",
    "mpl.rc('axes',unicode_minus= True )\n",
    "# 그래프 선명하게 출력\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import re\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# LSTM\n",
    "import tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "from tensorflow.keras.layers import Dropout, Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIMuPDLIo0rb"
   },
   "source": [
    "# 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJ4zJTn7o0LO"
   },
   "outputs": [],
   "source": [
    "df_data1 = pd.read_excel('/content/drive/MyDrive/data1.xlsx', engine='openpyxl')\n",
    "df_data2 = pd.read_excel('/content/drive/MyDrive/data2.xlsx', engine='openpyxl')\n",
    "df_stopword = pd.read_csv('/content/drive/MyDrive/stopword.txt', header = None, names = ['불용어'])\n",
    "\n",
    "df_test = pd.read_excel('/content/drive/MyDrive/input_data.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTVIEOA0Swkd"
   },
   "source": [
    "# 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxipO2o5kOtL"
   },
   "outputs": [],
   "source": [
    "# 데이터 병합\n",
    "df = pd.concat([df_data1, df_data2], sort = False)\n",
    "df = df.sort_values(by = ['NO'])\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "# 발화 1개 여러개 분리\n",
    "cond1 = df['발화2'].isnull()\n",
    "df_one = df[cond1]\n",
    "df_many = df[~cond1]\n",
    "\n",
    "\n",
    "# 분류 수정\n",
    "df1 = df_one[['NO', 'NO2', '발화', '발화1', '발화1의 우선순위']]\n",
    "df1 = df1.reset_index(drop = True)\n",
    "df1['수정된 분류'] = df1['발화1']\n",
    "\n",
    "cond2 = df1['수정된 분류'] == '중립'\n",
    "cond3 = df1['수정된 분류'] == '폐기'\n",
    "df1_neutral = df1[cond2]\n",
    "df1_discard = df1[cond3]\n",
    "\n",
    "df1_neutral_good = df1_neutral[df1_neutral.columns]\n",
    "df1_neutral_bad = df1_neutral[df1_neutral.columns]\n",
    "\n",
    "df1_neutral_good['수정된 분류'] = '칭찬>기타>중립'\n",
    "df1_neutral_bad['수정된 분류'] = '불만>기타>중립'\n",
    "\n",
    "df1_1 = df1[~cond2]\n",
    "df1_2 = pd.concat([df1_1, df1_neutral_good], sort = False)\n",
    "df2 = pd.concat([df1_2, df1_neutral_bad], sort = False)\n",
    "df2 = df2.reset_index(drop = True)\n",
    "\n",
    "df2.loc[df2['수정된 분류']=='칭찬>고객서비스', '수정된 분류'] = '칭찬>고객서비스>고객서비스'\n",
    "df2.loc[df2['수정된 분류']=='칭찬>삼성카드', '수정된 분류'] = '칭찬>삼성카드>삼성카드'\n",
    "df2.loc[df2['수정된 분류']=='칭찬>기타', '수정된 분류'] = '칭찬>기타>기타'\n",
    "df2.loc[df2['수정된 분류']=='불만>고객서비스', '수정된 분류'] = '불만>고객서비스>고객서비스'\n",
    "df2.loc[df2['수정된 분류']=='불만>삼성카드', '수정된 분류'] = '불만>삼성카드>삼성카드'\n",
    "df2.loc[df2['수정된 분류']=='불만>기타', '수정된 분류'] = '불만>기타>기타'\n",
    "\n",
    "\n",
    "# 레이블링\n",
    "df3 = df2[df2.columns]\n",
    "df3[['분류1','분류2','분류3']] = df3['수정된 분류'].str.split('>',n=3, expand=True)\n",
    "\n",
    "df4 = df3[df3.columns]\n",
    "df4['document'] = df4['발화']\n",
    "\n",
    "dic1 = {'칭찬' : 0, '불만' : 1}\n",
    "dic2 = {'고객서비스' : 0, '삼성카드' : 1, '기타' : 2}\n",
    "dic3_0 = {'상담원' : 0, '상담시스템' : 1, '고객서비스' : 2}\n",
    "dic3_1 = {'혜택' : 0, '할부금융상품' : 1, '커뮤니티서비스' : 2, '카드이용/결제' : 3, \n",
    "          '카드상품' : 4, '청구입금' : 5, '심사/한도' : 6, '생활편의서비스' : 7, \n",
    "          '상담/채널' : 8, '리스렌탈상품' : 9, '라이프서비스' : 10, '금융상품' : 11, \n",
    "          '고객정보관리' : 12, '가맹점매출/승인' : 13, '가맹점대금' : 14, '가맹점계약' : 15, '삼성카드' : 16}\n",
    "dic3_2 = {'기타' : 0, '중립' : 1, '폐기' : 2}\n",
    "\n",
    "for i in df4.index :\n",
    "    df4.loc[i, 'label1'] = dic1[df4.loc[i, '분류1']]\n",
    "    df4.loc[i, 'label2'] = dic2[df4.loc[i, '분류2']]\n",
    "    if df4.loc[i, '분류2'] == '고객서비스' :\n",
    "        df4.loc[i, 'label3'] = dic3_0[df4.loc[i, '분류3']]\n",
    "    elif df4.loc[i, '분류2'] == '삼성카드' :\n",
    "        df4.loc[i, 'label3'] = dic3_1[df4.loc[i, '분류3']]\n",
    "    elif df4.loc[i, '분류2'] == '기타' :\n",
    "        df4.loc[i, 'label3'] = dic3_2[df4.loc[i, '분류3']]\n",
    "    else :\n",
    "        print(i)\n",
    "        \n",
    "df4[['label1', 'label2', 'label3']] = df4[['label1', 'label2', 'label3']].astype('int')        \n",
    "\n",
    "\n",
    "# 맞춤법 교정을 위한\n",
    "# 특수문자 삭제\n",
    "df5 = df4[df4.columns]\n",
    "df5[\"document\"] = df5[\"document\"].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "\n",
    "# 이중 space 제거\n",
    "df6 = df5[df5.columns]\n",
    "new_document = []\n",
    "for sent in df6['document'] :\n",
    "    changed_sent = re.sub(' +', ' ', sent)\n",
    "    new_document.append(changed_sent)\n",
    "\n",
    "df6['document'] = new_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTdzKkFHkSdG"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "df7 = df6[df6.columns]\n",
    "\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "changed_document = []\n",
    "i = 0\n",
    "for sent in df7['document'] :\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    i += 1\n",
    "    \n",
    "    spelled_sent = spell_checker.check(sent)\n",
    "    checked_sent = spelled_sent.checked\n",
    "    changed_document.append(checked_sent)\n",
    "    \n",
    "    print('진행 상황 : ', i,'/',len(df7['document']))\n",
    "    print('진행 시간 : ', round((time.time() - start),2))\n",
    "\n",
    "print('총 걸린 시간 : ', round((time.time() - start),2), '초')\n",
    "\n",
    "df7['document'] = changed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI4pTckekU8u"
   },
   "outputs": [],
   "source": [
    "# 1차 분류를 위한 dataset\n",
    "cond11 = (df7['분류3'] == '중립')\n",
    "df_class_1 = df7.loc[~cond11][['document', 'label1']]\n",
    "\n",
    "# 2차 분류를 위한 dataset\n",
    "cond12 = (df7['분류3'] == '중립' )&(df7['분류1'] == '칭찬' )\n",
    "df_class_2 = df7.loc[~cond12][['document', 'label2']]\n",
    "\n",
    "# 2차 분류를 위한 dataset - 1차분류 결과로 따로 할 경우\n",
    "cond13 = (df7['분류1'] == '칭찬')\n",
    "cond14 = (df7['분류1'] == '불만')\n",
    "df_class_2_0 = df7.loc[cond13][['document', 'label2']]\n",
    "df_class_2_1 = df7.loc[cond14][['document', 'label2']]\n",
    "\n",
    "# 3차 분류를 위한 dataset\n",
    "cond15 = (df7['분류2'] == '고객서비스')\n",
    "cond16 = (df7['분류2'] == '삼성카드')\n",
    "cond17 = (df7['분류2'] == '기타')\n",
    "cond18 = (df7['분류3'] == '중립' )&(df7['분류1'] == '칭찬' )\n",
    "df_class_3_0 = df7.loc[cond15][['document', 'label3']]\n",
    "df_class_3_1 = df7.loc[cond16][['document', 'label3']]\n",
    "df_class_3_2 = df7.loc[cond17].loc[~cond18][['document', 'label3']]\n",
    "# print(len(df_class_3_0), len(df_class_3_1), len(df_class_3_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKoGdzLBkWzG"
   },
   "outputs": [],
   "source": [
    "# 시드값 설정 \n",
    "seed = 6124\n",
    "np.random.seed(seed)\n",
    "\n",
    "x_train_1 , x_test_1, y_train_1, y_test_1 = train_test_split(df_class_1['document'], \n",
    "                                                             df_class_1['label1'], \n",
    "                                                             test_size = 0.2, \n",
    "                                                             random_state = seed)\n",
    "\n",
    "x_train_2 , x_test_2, y_train_2, y_test_2 = train_test_split(df_class_2['document'], \n",
    "                                                             df_class_2['label2'], \n",
    "                                                             test_size = 0.2, \n",
    "                                                             random_state = seed)\n",
    "\n",
    "x_train_3_0 , x_test_3_0 , y_train_3_0 , y_test_3_0  = train_test_split(df_class_3_0['document'],\n",
    "                                                                        df_class_3_0['label3'], \n",
    "                                                                        test_size = 0.2, \n",
    "                                                                        random_state = seed)\n",
    "\n",
    "x_train_3_1 , x_test_3_1 , y_train_3_1 , y_test_3_1  = train_test_split(df_class_3_1['document'],\n",
    "                                                                        df_class_3_1['label3'], \n",
    "                                                                        test_size = 0.2, \n",
    "                                                                        random_state = seed)\n",
    "\n",
    "x_train_3_2 , x_test_3_2 , y_train_3_2 , y_test_3_2  = train_test_split(df_class_3_2['document'],\n",
    "                                                                        df_class_3_2['label3'], \n",
    "                                                                        test_size = 0.2, \n",
    "                                                                        random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_06luJzokY_X"
   },
   "outputs": [],
   "source": [
    "# 불용어 및 특수 기호 처리함수 구성 \n",
    "def stopword_function(X):\n",
    "    clean_train_review = []\n",
    "    okt = Okt()\n",
    "    # 모든 X 값에 대해 불용어 및 특수기호 처리 \n",
    "    for i in X:\n",
    "\n",
    "        # 문자데이터에 대한 불용어 처리 \n",
    "        if type(i) == str:\n",
    "            # 한글 자음, 모음 처리\n",
    "            review_text = re.sub(\"([ㄱ-ㅎㅏ-ㅣ]+)\", \"\", i) \n",
    "            # 특수 문자 처리 \n",
    "            review_text = re.sub(\"[^\\w\\s]\", \"\", review_text) \n",
    "            # Tokenizing & Stemming \n",
    "            word_text = okt.morphs(review_text,stem=True)\n",
    "            # StopWord \n",
    "            word_text = [token for token in word_text if not token in stopword_set]  \n",
    "\n",
    "            clean_train_review.append(word_text)\n",
    "\n",
    "        # 숫자데이터가 오는 경우, 공백을 append     \n",
    "        else:\n",
    "            clean_train_review.append([])\n",
    "            \n",
    "    return clean_train_review\n",
    "\n",
    "\n",
    "# 이중 리스트에서 해당 요소와 그 앞뒤 요소 출력하는 함수\n",
    "def find_letter_with_before_and_after(double_list, x) :\n",
    "    i = 0\n",
    "    for text in double_list :\n",
    "        if str(x) in text :\n",
    "            i += 1\n",
    "            if text.index(str(x)) == 0 :\n",
    "                print(text[text.index(str(x))], text[text.index(str(x))+1])\n",
    "            elif text.index(str(x)) >= len(text)-1 :\n",
    "                print(text[text.index(str(x))-1],text[text.index(str(x))])\n",
    "            else :\n",
    "                print(text[text.index(str(x))-1],text[text.index(str(x))], text[text.index(str(x))+1])\n",
    "    print('총, ', i, '개')\n",
    "\n",
    "    \n",
    "# 리스트의 특정 요소 인덱스 모두 찾는 함수\n",
    "def find_index(data, target):\n",
    "    res = []\n",
    "    lis = data\n",
    "    while True:\n",
    "        try:\n",
    "            res.append(lis.index(target) + (res[-1]+1 if len(res)!=0 else 0))\n",
    "            lis = data[res[-1]+1:]\n",
    "        except:\n",
    "            break     \n",
    "    return res\n",
    "\n",
    "\n",
    "# 리스트의 특정 요소 모두 삭제하는 함수\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "\n",
    "# 이중 리스트에서 두 단어를 합치는 함수 \n",
    "def combine_two_words(double_list, word1, word2) :\n",
    "    c_double_list = double_list\n",
    "    for i in range(len(c_double_list)) :\n",
    "        if str(word1) in c_double_list[i] :\n",
    "            for j in find_index(c_double_list[i], str(word1)) :\n",
    "                if j < len(c_double_list[i])-1 :\n",
    "                    if c_double_list[i][j+1] == str(word2) :\n",
    "                        c_double_list[i][j] = str(word1)+str(word2)\n",
    "                        c_double_list[i][j+1] = '삭제할 문자'\n",
    "            \n",
    "            c_double_list[i] = remove_values_from_list(c_double_list[i], '삭제할 문자')\n",
    "            \n",
    "    return c_double_list\n",
    "\n",
    "\n",
    "# 이중 리스트에서 한 단어 삭제하는 함수\n",
    "def remove_one_word(double_list, word) :\n",
    "    r_double_list = double_list\n",
    "    for i in range(len(r_double_list)) :\n",
    "        if str(word) in r_double_list[i] :\n",
    "            r_double_list[i] = remove_values_from_list(r_double_list[i], str(word))\n",
    "\n",
    "    return r_double_list\n",
    "\n",
    "\n",
    "# 이중 리스트에서 한 단어 변경하는 함수\n",
    "def change_one_word(double_list, word, changed_word) :\n",
    "    co_double_list = double_list\n",
    "    for i in range(len(co_double_list)) :\n",
    "        if str(word) in co_double_list[i] :\n",
    "            for j in find_index(co_double_list[i], str(word)) :\n",
    "                co_double_list[i][j] = str(changed_word)\n",
    "\n",
    "    return co_double_list\n",
    "\n",
    "\n",
    "# 추가 전처리\n",
    "def additional_cleaning(double_list) :\n",
    "    \n",
    "    # 원\n",
    "    double_list = combine_two_words(double_list, '상담', '원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상당', '원')\n",
    "    double_list = change_one_word(double_list, '상당원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '안내', '원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '원', '치')\n",
    "    double_list = change_one_word(double_list, '원치', '원하지')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상사', '원')\n",
    "    double_list = change_one_word(double_list, '상사원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상대', '원')\n",
    "    double_list = change_one_word(double_list, '상대원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '칙', '원')\n",
    "    double_list = change_one_word(double_list, '칙원', '직원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '칙', '원')\n",
    "\n",
    "    double_list = change_one_word(double_list, '결시', '연결')\n",
    "    double_list = change_one_word(double_list, '간이', '시간')\n",
    "\n",
    "    # 지\n",
    "    double_list = combine_two_words(double_list, '지', '원금')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '연')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '알', '지')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '양')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '양해')\n",
    "    double_list = change_one_word(double_list, '지양해', '지양')\n",
    "    \n",
    "    # 대\n",
    "    double_list = combine_two_words(double_list, '대', '기')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '체적')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '출사')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '화법')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '금도')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '현', '대')\n",
    "\n",
    "    # OOO\n",
    "    double_list = remove_one_word(double_list, 'OOO')\n",
    "    \n",
    "    return double_list\n",
    "\n",
    "\n",
    "# 이중 리스트 복제 함수\n",
    "def copy_double_list(double_list) : \n",
    "    copied_double_list = []\n",
    "    for i in range(len(double_list)) :\n",
    "        line = []\n",
    "        for j in range(len(double_list[i])):\n",
    "            line.append(double_list[i][j])\n",
    "        copied_double_list.append(line)\n",
    "        \n",
    "    return copied_double_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVyAje_BkamZ"
   },
   "outputs": [],
   "source": [
    "stopword_set = set(df_stopword['불용어'].values.tolist())\n",
    "\n",
    "x_train_1_clean = stopword_function(x_train_1)\n",
    "x_train_1_clean = additional_cleaning(x_train_1_clean)\n",
    "\n",
    "x_train_2_clean = stopword_function(x_train_2)\n",
    "x_train_2_clean = additional_cleaning(x_train_2_clean)\n",
    "\n",
    "x_train_3_0_clean = stopword_function(x_train_3_0)\n",
    "x_train_3_0_clean = additional_cleaning(x_train_3_0_clean)\n",
    "x_train_3_1_clean = stopword_function(x_train_3_1)\n",
    "x_train_3_1_clean = additional_cleaning(x_train_3_1_clean)\n",
    "x_train_3_2_clean = stopword_function(x_train_3_2)\n",
    "x_train_3_2_clean = additional_cleaning(x_train_3_2_clean)\n",
    "\n",
    "\n",
    "x_test_1_clean = stopword_function(x_test_1)\n",
    "x_test_1_clean = additional_cleaning(x_test_1_clean)\n",
    "\n",
    "x_test_2_clean = stopword_function(x_test_2)\n",
    "x_test_2_clean = additional_cleaning(x_test_2_clean)\n",
    "\n",
    "x_test_3_0_clean = stopword_function(x_test_3_0)\n",
    "x_test_3_0_clean = additional_cleaning(x_test_3_0_clean)\n",
    "x_test_3_1_clean = stopword_function(x_test_3_1)\n",
    "x_test_3_1_clean = additional_cleaning(x_test_3_1_clean)\n",
    "x_test_3_2_clean = stopword_function(x_test_3_2)\n",
    "x_test_3_2_clean = additional_cleaning(x_test_3_2_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3mwXiYEkclI"
   },
   "outputs": [],
   "source": [
    "tensorflow.compat.v1.set_random_seed(196)\n",
    "\n",
    "vocab_size_1 = 2000\n",
    "model_token_1 = Tokenizer(num_words = vocab_size_1)\n",
    "model_token_1.fit_on_texts(x_train_1_clean)\n",
    "x_sequences_1 = model_token_1.texts_to_sequences(x_train_1_clean)\n",
    "max_length_1 = 80\n",
    "train_x_1 = pad_sequences(x_sequences_1, maxlen = max_length_1, padding = 'post')\n",
    "train_y_1 = to_categorical(y_train_1.astype(int))\n",
    "\n",
    "\n",
    "vocab_size_2 = 2000\n",
    "model_token_2 = Tokenizer(num_words = vocab_size_2)\n",
    "model_token_2.fit_on_texts(x_train_2_clean)\n",
    "x_sequences_2 = model_token_2.texts_to_sequences(x_train_2_clean)\n",
    "max_length_2 = 80\n",
    "train_x_2 = pad_sequences(x_sequences_2, maxlen = max_length_2, padding = 'post')\n",
    "train_y_2 = to_categorical(y_train_2.astype(int))\n",
    "\n",
    "\n",
    "vocab_size_3_0 = 2000\n",
    "model_token_3_0 = Tokenizer(num_words = vocab_size_3_0)\n",
    "model_token_3_0.fit_on_texts(x_train_3_0_clean)\n",
    "x_sequences_3_0 = model_token_3_0.texts_to_sequences(x_train_3_0_clean)\n",
    "max_length_3_0 = 80\n",
    "train_x_3_0 = pad_sequences(x_sequences_3_0, maxlen = max_length_3_0, padding = 'post')\n",
    "train_y_3_0 = to_categorical(y_train_3_0.astype(int))\n",
    "\n",
    "vocab_size_3_1 = 2000\n",
    "model_token_3_1 = Tokenizer(num_words = vocab_size_3_1)\n",
    "model_token_3_1.fit_on_texts(x_train_3_1_clean)\n",
    "x_sequences_3_1 = model_token_3_1.texts_to_sequences(x_train_3_1_clean)\n",
    "max_length_3_1 = 80\n",
    "train_x_3_1 = pad_sequences(x_sequences_3_1, maxlen = max_length_3_1, padding = 'post')\n",
    "train_y_3_1 = to_categorical(y_train_3_1.astype(int))\n",
    "\n",
    "vocab_size_3_2 = 2000\n",
    "model_token_3_2 = Tokenizer(num_words = vocab_size_3_2)\n",
    "model_token_3_2.fit_on_texts(x_train_3_2_clean)\n",
    "x_sequences_3_2 = model_token_3_2.texts_to_sequences(x_train_3_2_clean)\n",
    "max_length_3_2 = 80\n",
    "train_x_3_2 = pad_sequences(x_sequences_3_2, maxlen = max_length_3_2, padding = 'post')\n",
    "train_y_3_2 = to_categorical(y_train_3_2.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi56zpLAxIh7"
   },
   "outputs": [],
   "source": [
    "vocab_size_2 = 2000\n",
    "model_token_2_t = Tokenizer(num_words = vocab_size_2)\n",
    "model_token_2_t.fit_on_texts(x_test_2_clean)\n",
    "x_sequences_2_t = model_token_2_t.texts_to_sequences(x_test_2_clean)\n",
    "max_length_2 = 80\n",
    "test_x_2 = pad_sequences(x_sequences_2_t, maxlen = max_length_2, padding = 'post')\n",
    "test_y_2 = to_categorical(y_test_2.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0AOEQY6kpR0"
   },
   "outputs": [],
   "source": [
    "def metrics_plot(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs =range(1, len(acc)+1)\n",
    "\n",
    "    plt.plot(epochs, acc, label='Train Acc')\n",
    "    plt.plot(epochs, val_acc, label='Validation Acc')\n",
    "    plt.title('Accuracy Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss , label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Loss Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjY-GVqOkpUT"
   },
   "outputs": [],
   "source": [
    "# 1차 분류 모델\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# 기본 Bert tokenizer 사용\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpfSMcbTkpWx"
   },
   "outputs": [],
   "source": [
    "# 1차분류를 위한 발화 + label로 이루어진 리스트 신규 생성\n",
    "# x_train_1_clean & y_train_1_clean 은 불용어 + 토크나이징을 동시에 처리된 데이터셋이고\n",
    "# 1차분류에서는 따로 BERT Tokenizer를 쓸 예정이기에 토크나이징이 안되어있는 x_train_1 & y_train_1 사용\n",
    "\n",
    "train_1_list = []\n",
    "test_1_list = []\n",
    "\n",
    "for q, label in zip(x_train_1, y_train_1)  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    train_1_list.append(data)\n",
    "\n",
    "for q, label in zip(x_test_1, y_test_1)  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    test_1_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEgloWyhkpZU"
   },
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "data_train = BERTDataset(train_1_list, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test_1_list, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# pytorch용 DataLoader 사용\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 2, \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "  \n",
    "# Bert 모델 불러오기\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "train_dataloader\n",
    "\n",
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "   \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b11sDfgQkpbv"
   },
   "outputs": [],
   "source": [
    "# 2차분류를 위한 발화 + label로 이루어진 리스트 신규 생성\n",
    "# x_train_2_clean & y_train_2_clean 은 불용어 + 토크나이징을 동시에 처리된 데이터셋이고\n",
    "# 2차분류에서는 따로 BERT Tokenizer를 쓸 예정이기에 토크나이징이 안되어있는 x_train_2 & y_train_2 사용\n",
    "\n",
    "train_2_list = []\n",
    "test_2_list = []\n",
    "\n",
    "for q, label in zip(x_train_2, y_train_2)  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    train_2_list.append(data)\n",
    "\n",
    "for q, label in zip(x_test_2, y_test_2)  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    test_2_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuGP1AVLkpeO"
   },
   "outputs": [],
   "source": [
    "# 2차 분류 모델\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "data_train = BERTDataset(train_2_list, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test_2_list, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# pytorch용 DataLoader 사용\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 3, \n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "  \n",
    "# Bert 모델 불러오기\n",
    "model2 = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model2.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model2.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 정확도측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "train_dataloader\n",
    "\n",
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model2.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model2(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "   \n",
    "    model2.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model2(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5tvcSv3kpgV"
   },
   "outputs": [],
   "source": [
    "# 3-0차 분류 모델\n",
    "earlystop_callback = EarlyStopping(monitor = 'val_acc', min_delta = 0.0001, patience = 5)\n",
    "\n",
    "model_3_0 = Sequential([Embedding(vocab_size_3_0, 300, input_length =max_length_3_0),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model_3_0.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_3_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQfq9GHbkpik"
   },
   "outputs": [],
   "source": [
    "history_3_0 = model_3_0.fit(train_x_3_0, train_y_3_0,\n",
    "                        epochs=40, batch_size=64, validation_split=0.2, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuk6ik8ykplH"
   },
   "outputs": [],
   "source": [
    "metrics_plot(history_3_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psf58FZ_xX_5"
   },
   "outputs": [],
   "source": [
    "# 3-1차 분류 모델\n",
    "earlystop_callback = EarlyStopping(monitor = 'val_acc', min_delta = 0.0001, patience = 5)\n",
    "\n",
    "model_3_1 = Sequential([Embedding(vocab_size_3_1, 300, input_length =max_length_3_1),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "        Dense(17, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model_3_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ty91yzGxYCE"
   },
   "outputs": [],
   "source": [
    "history_3_1 = model_3_1.fit(train_x_3_1, train_y_3_1,\n",
    "                        epochs=40, batch_size=64, validation_split=0.2, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBRWtL4kkpnp"
   },
   "outputs": [],
   "source": [
    "metrics_plot(history_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "628CxcMTkpqY"
   },
   "outputs": [],
   "source": [
    "# 3-2차 분류 모델\n",
    "earlystop_callback = EarlyStopping(monitor = 'val_acc', min_delta = 0.0001, patience = 5)\n",
    "\n",
    "model_3_2 = Sequential([Embedding(vocab_size_3_2, 300, input_length =max_length_3_2),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model_3_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_3_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB3Rpbk0kptk"
   },
   "outputs": [],
   "source": [
    "history_3_2 = model_3_2.fit(train_x_3_2, train_y_3_2,\n",
    "                        epochs=40, batch_size=64, validation_split=0.2, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkZ_xpWVxfO_"
   },
   "outputs": [],
   "source": [
    "metrics_plot(history_3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZX6jC22S99-"
   },
   "source": [
    "# 모델 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLDvBcGMSdwg"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_excel('/content/drive/MyDrive/input_data.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zqhXvByxn91"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    " # 시작 시간 저장\n",
    "start = time.time() \n",
    "\n",
    "# 맞춤법 교정을 위한 특수문자 삭제\n",
    "df_test[\"TEXT_cleaned\"] = df_test[\"TEXT\"].str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "\n",
    "# 맞춤법 교정을 위한이중 space 제거\n",
    "new_document = []\n",
    "for sent in df_test[\"TEXT_cleaned\"] :\n",
    "    changed_sent = re.sub(' +', ' ', sent)\n",
    "    new_document.append(changed_sent)\n",
    "\n",
    "df_test[\"TEXT_cleaned\"] = new_document\n",
    "\n",
    "\n",
    "# 맞춤법 교정\n",
    "changed_document = []\n",
    "i = 0\n",
    "for sent in df_test[\"TEXT_cleaned\"] :\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    i += 1\n",
    "    \n",
    "    spelled_sent = spell_checker.check(sent)\n",
    "    checked_sent = spelled_sent.checked\n",
    "    changed_document.append(checked_sent)\n",
    "    \n",
    "    print('진행 상황 : ', i,'/',len(df_test[\"TEXT_cleaned\"]))\n",
    "    print('진행 시간 : ', round((time.time() - start),2))\n",
    "\n",
    "print('총 걸린 시간 : ', round((time.time() - start),2), '초')\n",
    "\n",
    "df_test[\"TEXT_cleaned\"] = changed_document\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3nSRzHCxoAt"
   },
   "outputs": [],
   "source": [
    "# 버트용\n",
    "test_input_serires = df_test[\"TEXT_cleaned\"]\n",
    "test_input_serires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_phwxCExoCw"
   },
   "outputs": [],
   "source": [
    "def stopword_function(X):\n",
    "    clean_train_review = []\n",
    "    okt = Okt()\n",
    "    # 모든 X 값에 대해 불용어 및 특수기호 처리 \n",
    "    for i in X:\n",
    "\n",
    "        # 문자데이터에 대한 불용어 처리 \n",
    "        if type(i) == str:\n",
    "            # 한글 자음, 모음 처리\n",
    "            review_text = re.sub(\"([ㄱ-ㅎㅏ-ㅣ]+)\", \"\", i) \n",
    "            # 특수 문자 처리 \n",
    "            review_text = re.sub(\"[^\\w\\s]\", \"\", review_text) \n",
    "            # Tokenizing & Stemming \n",
    "            word_text = okt.morphs(review_text,stem=True)\n",
    "            # StopWord \n",
    "            word_text = [token for token in word_text if not token in stopword_set]  \n",
    "\n",
    "            clean_train_review.append(word_text)\n",
    "\n",
    "        # 숫자데이터가 오는 경우, 공백을 append     \n",
    "        else:\n",
    "            clean_train_review.append([])\n",
    "            \n",
    "    return clean_train_review\n",
    "\n",
    "\n",
    "# 이중 리스트에서 해당 요소와 그 앞뒤 요소 출력하는 함수\n",
    "def find_letter_with_before_and_after(double_list, x) :\n",
    "    i = 0\n",
    "    for text in double_list :\n",
    "        if str(x) in text :\n",
    "            i += 1\n",
    "            if text.index(str(x)) == 0 :\n",
    "                print(text[text.index(str(x))], text[text.index(str(x))+1])\n",
    "            elif text.index(str(x)) >= len(text)-1 :\n",
    "                print(text[text.index(str(x))-1],text[text.index(str(x))])\n",
    "            else :\n",
    "                print(text[text.index(str(x))-1],text[text.index(str(x))], text[text.index(str(x))+1])\n",
    "    print('총, ', i, '개')\n",
    "\n",
    "    \n",
    "# 리스트의 특정 요소 인덱스 모두 찾는 함수\n",
    "def find_index(data, target):\n",
    "    res = []\n",
    "    lis = data\n",
    "    while True:\n",
    "        try:\n",
    "            res.append(lis.index(target) + (res[-1]+1 if len(res)!=0 else 0))\n",
    "            lis = data[res[-1]+1:]\n",
    "        except:\n",
    "            break     \n",
    "    return res\n",
    "\n",
    "\n",
    "# 리스트의 특정 요소 모두 삭제하는 함수\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "\n",
    "# 이중 리스트에서 두 단어를 합치는 함수 \n",
    "def combine_two_words(double_list, word1, word2) :\n",
    "    c_double_list = double_list\n",
    "    for i in range(len(c_double_list)) :\n",
    "        if str(word1) in c_double_list[i] :\n",
    "            for j in find_index(c_double_list[i], str(word1)) :\n",
    "                if j < len(c_double_list[i])-1 :\n",
    "                    if c_double_list[i][j+1] == str(word2) :\n",
    "                        c_double_list[i][j] = str(word1)+str(word2)\n",
    "                        c_double_list[i][j+1] = '삭제할 문자'\n",
    "            \n",
    "            c_double_list[i] = remove_values_from_list(c_double_list[i], '삭제할 문자')\n",
    "            \n",
    "    return c_double_list\n",
    "\n",
    "\n",
    "# 이중 리스트에서 한 단어 삭제하는 함수\n",
    "def remove_one_word(double_list, word) :\n",
    "    r_double_list = double_list\n",
    "    for i in range(len(r_double_list)) :\n",
    "        if str(word) in r_double_list[i] :\n",
    "            r_double_list[i] = remove_values_from_list(r_double_list[i], str(word))\n",
    "\n",
    "    return r_double_list\n",
    "\n",
    "\n",
    "# 이중 리스트에서 한 단어 변경하는 함수\n",
    "def change_one_word(double_list, word, changed_word) :\n",
    "    co_double_list = double_list\n",
    "    for i in range(len(co_double_list)) :\n",
    "        if str(word) in co_double_list[i] :\n",
    "            for j in find_index(co_double_list[i], str(word)) :\n",
    "                co_double_list[i][j] = str(changed_word)\n",
    "\n",
    "    return co_double_list\n",
    "\n",
    "\n",
    "# 추가 전처리\n",
    "def additional_cleaning(double_list) :\n",
    "    \n",
    "    # 원\n",
    "    double_list = combine_two_words(double_list, '상담', '원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상당', '원')\n",
    "    double_list = change_one_word(double_list, '상당원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '안내', '원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '원', '치')\n",
    "    double_list = change_one_word(double_list, '원치', '원하지')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상사', '원')\n",
    "    double_list = change_one_word(double_list, '상사원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '상대', '원')\n",
    "    double_list = change_one_word(double_list, '상대원', '상담원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '칙', '원')\n",
    "    double_list = change_one_word(double_list, '칙원', '직원')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '칙', '원')\n",
    "\n",
    "    double_list = change_one_word(double_list, '결시', '연결')\n",
    "    double_list = change_one_word(double_list, '간이', '시간')\n",
    "\n",
    "    # 지\n",
    "    double_list = combine_two_words(double_list, '지', '원금')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '연')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '알', '지')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '양')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '지', '양해')\n",
    "    double_list = change_one_word(double_list, '지양해', '지양')\n",
    "    \n",
    "    # 대\n",
    "    double_list = combine_two_words(double_list, '대', '기')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '체적')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '출사')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '화법')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '대', '금도')\n",
    "\n",
    "    double_list = combine_two_words(double_list, '현', '대')\n",
    "\n",
    "    # OOO\n",
    "    double_list = remove_one_word(double_list, 'OOO')\n",
    "    \n",
    "    return double_list\n",
    "\n",
    "\n",
    "# 이중 리스트 복제 함수\n",
    "def copy_double_list(double_list) : \n",
    "    copied_double_list = []\n",
    "    for i in range(len(double_list)) :\n",
    "        line = []\n",
    "        for j in range(len(double_list[i])):\n",
    "            line.append(double_list[i][j])\n",
    "        copied_double_list.append(line)\n",
    "        \n",
    "    return copied_double_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAz68zKTxxCS"
   },
   "outputs": [],
   "source": [
    "stopword_set = set(df_stopword['불용어'].values.tolist())\n",
    "\n",
    "test_tok = stopword_function(df_test[\"TEXT_cleaned\"])\n",
    "test_tok = additional_cleaning(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmO6MK08xxJm"
   },
   "outputs": [],
   "source": [
    "tensorflow.compat.v1.set_random_seed(196)\n",
    "\n",
    "vocab_size_test = 2000\n",
    "\n",
    "model_token_test = Tokenizer(num_words = vocab_size_test)\n",
    "model_token_test.fit_on_texts(test_tok)\n",
    "\n",
    "x_sequences_test = model_token_test.texts_to_sequences(test_tok)\n",
    "\n",
    "max_length = 80\n",
    "test_input = pad_sequences(x_sequences_test, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkPPBRtkxzUc"
   },
   "outputs": [],
   "source": [
    "# 분류 1 진행 -> label_1 구해짐\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 위에서 설정한 tok, max_len, batch_size, device 그대로 사용\n",
    "# comment : 예측하고자 하는 테스트 데이터 발화 리스트\n",
    "def final_1_classification(comment, tok, max_len, batch_size, device):\n",
    "  commnetslist = [] # 발화 리스트\n",
    "  emo_list = [] # 1차 분류 값을 담을 리스트\n",
    "  for c in comment: # 발화 리스트\n",
    "    commnetslist.append( [c, 3] ) # [댓글, 임의의 양의 정수값] 설정\n",
    "    \n",
    "  pdData = pd.DataFrame( commnetslist, columns = [['발화', '1차분류']] )\n",
    "  pdData = pdData.values\n",
    "  test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "  test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=0)\n",
    "  \n",
    "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length \n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "\t\n",
    "    for e in out:\n",
    "      if e[0]>e[1]: \n",
    "        value = 0\n",
    "      else: \n",
    "        value = 1\n",
    "      emo_list.append(value)\n",
    "\n",
    "  return emo_list \n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "label_1 = final_1_classification(test_input_serires, tok, max_len, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O8folUqxoEf"
   },
   "outputs": [],
   "source": [
    "df_test['label1'] = label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooXFUfoex2Is"
   },
   "outputs": [],
   "source": [
    "# 분류 2 진행 -> label_2 구해짐\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 위에서 설정한 tok, max_len, batch_size, device 그대로 사용\n",
    "# comment : 예측하고자 하는 테스트 데이터 발화 리스트\n",
    "def final_2_classification(comment, tok, max_len, batch_size, device):\n",
    "  commnetslist = [] # 발화 리스트\n",
    "  emo_list = [] # 2차 분류 값을 담을 리스트\n",
    "  for c in comment: # 발화 리스트\n",
    "    commnetslist.append( [c, 4] ) # [댓글, 임의의 양의 정수값] 설정\n",
    "    \n",
    "  pdData = pd.DataFrame( commnetslist, columns = [['발화', '2차분류']] )\n",
    "  pdData = pdData.values\n",
    "  test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "  test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=0)\n",
    "  \n",
    "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length \n",
    "    out = model2(token_ids, valid_length, segment_ids)\n",
    "\n",
    "    for e in out:\n",
    "      if e[0]>e[1] and e[0]>e[2]: \n",
    "        value = 0\n",
    "      elif e[1]>e[0] and e[1]>e[2]:\n",
    "        value = 1\n",
    "      else:\n",
    "        value = 2\n",
    "      emo_list.append(value)\n",
    "\n",
    "  return emo_list \n",
    "   \n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "label_2 = final_2_classification(test_input_serires, tok, max_len, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwoL-mAxx7SV"
   },
   "outputs": [],
   "source": [
    "condt2_0 = (df_test['label2'] == 0)\n",
    "condt2_1 = (df_test['label2'] == 1)\n",
    "condt2_2 = (df_test['label2'] == 2)\n",
    "\n",
    "df_test_3_0 = df_test.loc[condt2_0]\n",
    "df_test_3_1 = df_test.loc[condt2_1]\n",
    "df_test_3_2 = df_test.loc[condt2_2]\n",
    "\n",
    "# 불용어 제거\n",
    "stopword_set = set(df_stopword['불용어'].values.tolist())\n",
    "\n",
    "# 토큰화\n",
    "test_tok_3_0 = stopword_function(df_test_3_0[\"TEXT_cleaned\"])\n",
    "test_tok_3_0 = additional_cleaning(test_tok_3_0)\n",
    "\n",
    "test_tok_3_1 = stopword_function(df_test_3_1[\"TEXT_cleaned\"])\n",
    "test_tok_3_1 = additional_cleaning(test_tok_3_1)\n",
    "\n",
    "test_tok_3_2 = stopword_function(df_test_3_2[\"TEXT_cleaned\"])\n",
    "test_tok_3_2 = additional_cleaning(test_tok_3_2)\n",
    "\n",
    "\n",
    "tensorflow.compat.v1.set_random_seed(196)\n",
    "\n",
    "vocab_size_test = 2000\n",
    "max_length = 80\n",
    "\n",
    "model_token_test_3_0 = Tokenizer(num_words = vocab_size_test)\n",
    "model_token_test_3_0.fit_on_texts(test_tok_3_0)\n",
    "x_sequences_test_3_0 = model_token_test_3_0.texts_to_sequences(test_tok_3_0)\n",
    "test_input_3_0 = pad_sequences(x_sequences_test_3_0, maxlen = max_length, padding = 'post')\n",
    "\n",
    "model_token_test_3_1 = Tokenizer(num_words = vocab_size_test)\n",
    "model_token_test_3_1.fit_on_texts(test_tok_3_1)\n",
    "x_sequences_test_3_1 = model_token_test_3_1.texts_to_sequences(test_tok_3_1)\n",
    "test_input_3_1 = pad_sequences(x_sequences_test_3_1, maxlen = max_length, padding = 'post')\n",
    "\n",
    "model_token_test_3_2 = Tokenizer(num_words = vocab_size_test)\n",
    "model_token_test_3_2.fit_on_texts(test_tok_3_2)\n",
    "x_sequences_test_3_2 = model_token_test_3_2.texts_to_sequences(test_tok_3_2)\n",
    "test_input_3_2 = pad_sequences(x_sequences_test_3_2, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEUSLIEzx7Ur"
   },
   "outputs": [],
   "source": [
    "# 분류3 진행 -> label_3_0, label_3_1, label_3_2 구해짐\n",
    "\n",
    "# 모델 3_0 으로 라벨 3_0 예측\n",
    "pred_3_0 = model_3_0.predict(test_input_3_0)\n",
    "label_3_0 = np.argmax(pred_3_0, axis = 1)\n",
    "\n",
    "# 모델 3_1 으로 라벨 3_1 예측\n",
    "pred_3_1 = model_3_1.predict(test_input_3_1)\n",
    "label_3_1 = np.argmax(pred_3_1, axis = 1)\n",
    "\n",
    "# 모델 3_2 으로 라벨 3_2 예측\n",
    "pred_3_2 = model_3_2.predict(test_input_3_2)\n",
    "label_3_2 = np.argmax(pred_3_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlJsMa6qx-x-"
   },
   "outputs": [],
   "source": [
    "df_test_3_0['label3'] = label_3_0\n",
    "df_test_3_1['label3'] = label_3_1\n",
    "df_test_3_2['label3'] = label_3_2\n",
    "\n",
    "df_result = pd.concat([df_test_3_0, df_test_3_1], sort = False)\n",
    "df_result = pd.concat([df_result, df_test_3_2], sort = False)\n",
    "\n",
    "df_result = df_result.sort_index()\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_avOVsp4x-0X"
   },
   "outputs": [],
   "source": [
    "df_result[['label1', 'label2', 'label3']] = df_result[['label1', 'label2', 'label3']].astype('int')        \n",
    "\n",
    "dic1 = {'칭찬' : 0, '불만' : 1}\n",
    "dic2 = {'고객서비스' : 0, '삼성카드' : 1, '기타' : 2}\n",
    "dic3_0 = {'상담원' : 0, '상담시스템' : 1, '고객서비스' : 2}\n",
    "dic3_1 = {'혜택' : 0, '할부금융상품' : 1, '커뮤니티서비스' : 2, '카드이용/결제' : 3, \n",
    "          '카드상품' : 4, '청구입금' : 5, '심사/한도' : 6, '생활편의서비스' : 7, \n",
    "          '상담/채널' : 8, '리스렌탈상품' : 9, '라이프서비스' : 10, '금융상품' : 11, \n",
    "          '고객정보관리' : 12, '가맹점매출/승인' : 13, '가맹점대금' : 14, '가맹점계약' : 15, '삼성카드' : 16}\n",
    "dic3_2 = {'기타' : 0, '중립' : 1, '폐기' : 2}\n",
    "\n",
    "dic1_reversed = {v:k for k, v in dic1.items()}\n",
    "dic2_reversed = {v:k for k, v in dic2.items()}\n",
    "dic3_0_reversed = {v:k for k, v in dic3_0.items()}\n",
    "dic3_1_reversed = {v:k for k, v in dic3_1.items()}\n",
    "dic3_2_reversed = {v:k for k, v in dic3_2.items()}\n",
    "\n",
    "for i in df_result.index :\n",
    "    df_result.loc[i, '분류1'] = dic1_reversed[df_result.loc[i, 'label1']]\n",
    "    df_result.loc[i, '분류2'] = dic2_reversed[df_result.loc[i, 'label2']]\n",
    "    if df_result.loc[i, 'label2'] == 0 :\n",
    "        df_result.loc[i, '분류3'] = dic3_0_reversed[df_result.loc[i, 'label3']]\n",
    "    elif df_result.loc[i, 'label2'] == 1 :\n",
    "        df_result.loc[i, '분류3'] = dic3_1_reversed[df_result.loc[i, 'label3']]\n",
    "    elif df_result.loc[i, 'label2'] == 2 :\n",
    "        df_result.loc[i, '분류3'] = dic3_2_reversed[df_result.loc[i, 'label3']]\n",
    "    else :\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "df_result['분류'] = df_result[['분류1', '분류2', '분류3']].apply('>'.join, axis=1)\n",
    "\n",
    "df_result['복구된 분류'] = df_result['분류']\n",
    "\n",
    "df_result.loc[df_result['복구된 분류']=='칭찬>고객서비스>고객서비스', '복구된 분류'] = '칭찬>고객서비스'\n",
    "df_result.loc[df_result['복구된 분류']=='칭찬>삼성카드>삼성카드', '복구된 분류'] = '칭찬>삼성카드'\n",
    "df_result.loc[df_result['복구된 분류']=='칭찬>기타>기타', '복구된 분류'] = '칭찬>기타'\n",
    "df_result.loc[df_result['복구된 분류']=='불만>고객서비스>고객서비스', '복구된 분류'] = '불만>고객서비스'\n",
    "df_result.loc[df_result['복구된 분류']=='불만>삼성카드>삼성카드', '복구된 분류'] = '불만>삼성카드'\n",
    "df_result.loc[df_result['복구된 분류']=='불만>기타>기타', '복구된 분류'] = '불만>기타'\n",
    "\n",
    "df_result.loc[df_result['복구된 분류']=='칭찬>기타>중립', '복구된 분류'] = '중립'\n",
    "df_result.loc[df_result['복구된 분류']=='불만>기타>중립', '복구된 분류'] = '중립'\n",
    "\n",
    "df_result['INT'] = df_result['복구된 분류']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcZM3ed3yGwz"
   },
   "outputs": [],
   "source": [
    "df_final_result = df_result[['KEY1', 'KEY2', 'TEXT', 'INT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f_NifWnpKw-"
   },
   "source": [
    "# 결과출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UctiMXI8UQ0Q"
   },
   "outputs": [],
   "source": [
    "df_final_result.to_excel('/content/drive/MyDrive/result_data.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "196번.흐르는강물을거꾸로거슬러오르는자연어들처럼(김민기,장우재,남웅찬).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
